using DelimitedFiles
text = "There is something regal in the antigravity air. Farming is an agricultural business, it does not use antigovermantal regulations."
terms = ["reg", "anti", "agri"]
output = []

function tokenize_by_root_words(original_text::String, terms::Array{String,1})
    text_by_spaces = split(original_text)
    println("text_by_spaces ", text_by_spaces)
    
    # Create a copy of text to prevent original text modification
    tokens = []
    
    # Iterate over terms
    for root in terms
        for word in text_by_spaces # TODO parallelize
            # If the term exists in the text
            while occursin(root, word)
                println("the root '", root,"' exists in word '", word, "' ")

                # Get the start and end indices of the term in the text
                start_ind, end_ind = findfirst(root, word)
                println("processing root '", root,"' in in word '", word, 
                "'  start_ind ", start_ind, " end_ind ", end_ind, " found '", word[start_ind:end_ind+1], "' " )
                return

                # If the term is not at the start of the text
                if start_ind > 1
                    # Get the token before the term
                    pre_token = strip(text_by_spaces[1:start_ind-1])
                    if pre_token != ""
                        push!(tokens, pre_token)
                    end
                end
                
                # Add the term to the tokens
                push!(tokens, term)
                
                # Remove the part of the text up to and including the term
                original_text = text_by_spaces[end_ind+1:end]
            end
        end
    end

    # Add any remaining tokens from the text
    remaining_tokens = split(text_by_spaces)
    append!(tokens, remaining_tokens)

    return tokens
end

# Testing the function

output = tokenize_by_root_words(text, terms)
println(output)





