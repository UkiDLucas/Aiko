{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64778a40-18a4-41d7-806d-af4c0d346144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "remove_timestamps (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()\n",
    "include(\"../julia/remove_timestamps.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a65e203f-3eac-4c70-a90c-43efebcef86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWEBVTT\\n\\n00:00.000 --> 00:02.800\\n By the time he gets to 2045,\\n\\n00:56.520 --> 00:58.320\\n This is the Lex Friedman podcast.\\n\\n00:58.320 --> 01:00.360\\n To support it, please check out our sponsors\\n\\n01:00.360 --> 01:01.640\\n in the description.\\n\\n01:01.640 --> 01:05.360\\n And \" â‹¯ 23372 bytes â‹¯ \" how you've developed that brain power\\n\\n1:07:24.360 --> 1:07:26.720\\n to start to think in a futurist sense\\n\\n1:07:26.720 --> 1:07:31.040\\n when how will the world look like in 2045\\n\\n1:35:58.460 --> 1:36:21.460\\n Thank you for listening, and hope to see you next time.\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \n",
    "\"\n",
    "WEBVTT\n",
    "\n",
    "00:00.000 --> 00:02.800\n",
    " By the time he gets to 2045,\n",
    "\n",
    "00:56.520 --> 00:58.320\n",
    " This is the Lex Friedman podcast.\n",
    "\n",
    "00:58.320 --> 01:00.360\n",
    " To support it, please check out our sponsors\n",
    "\n",
    "01:00.360 --> 01:01.640\n",
    " in the description.\n",
    "\n",
    "01:01.640 --> 01:05.360\n",
    " And now, dear friends, here's Ray Kurzweil.\n",
    "\n",
    "01:06.360 --> 01:10.960\n",
    " In your 2005 book titled The Singularity is Near,\n",
    "\n",
    "01:10.960 --> 01:15.400\n",
    " you predicted that the singularity will happen in 2045.\n",
    "\n",
    "01:15.400 --> 01:18.460\n",
    " So now, 18 years later, do you still estimate\n",
    "\n",
    "01:18.460 --> 01:22.480\n",
    " that the singularity will happen on 2045?\n",
    "\n",
    "01:22.480 --> 01:24.960\n",
    " And maybe first, what is the singularity,\n",
    "\n",
    "01:24.960 --> 01:27.760\n",
    " the technological singularity, and when will it happen?\n",
    "\n",
    "01:27.760 --> 01:31.640\n",
    " Singularity is where computers really change our view\n",
    "\n",
    "01:31.640 --> 01:35.840\n",
    " of what's important and change who we are.\n",
    "\n",
    "01:35.840 --> 01:39.560\n",
    " But we're getting close to some salient things\n",
    "\n",
    "01:39.560 --> 01:42.800\n",
    " that will change who we are.\n",
    "\n",
    "01:42.800 --> 01:45.680\n",
    " A key thing is 2029,\n",
    "\n",
    "01:45.680 --> 01:49.060\n",
    " when computers will pass the Turing test.\n",
    "\n",
    "01:50.120 --> 01:51.520\n",
    " And there's also some controversy\n",
    "\n",
    "01:51.520 --> 01:53.680\n",
    " whether the Turing test is valid.\n",
    "\n",
    "01:53.680 --> 01:55.080\n",
    " I believe it is.\n",
    "\n",
    "01:55.080 --> 01:57.920\n",
    " Most people do believe that,\n",
    "\n",
    "01:57.920 --> 01:59.680\n",
    " but there's some controversy about that.\n",
    "\n",
    "01:59.680 --> 02:04.680\n",
    " But Stanford got very alarmed at my prediction about 2029.\n",
    "\n",
    "02:06.520 --> 02:10.520\n",
    " I made this in 1999 in my book.\n",
    "\n",
    "02:10.520 --> 02:12.120\n",
    " The Age of Spiritual Machines.\n",
    "\n",
    "02:12.120 --> 02:12.960\n",
    " Right.\n",
    "\n",
    "02:12.960 --> 02:15.600\n",
    " And then you repeated the prediction in 2005.\n",
    "\n",
    "02:15.600 --> 02:16.600\n",
    " In 2005.\n",
    "\n",
    "02:16.600 --> 02:17.520\n",
    " Yeah.\n",
    "\n",
    "02:17.520 --> 02:19.480\n",
    " So they held an international conference,\n",
    "\n",
    "02:19.480 --> 02:20.800\n",
    " you might have been aware of it,\n",
    "\n",
    "02:20.800 --> 02:25.800\n",
    " of AI experts in 1999 to assess this view.\n",
    "\n",
    "02:26.620 --> 02:29.580\n",
    " So people gave different predictions,\n",
    "\n",
    "02:29.580 --> 02:30.840\n",
    " and they took a poll.\n",
    "\n",
    "02:30.840 --> 02:34.240\n",
    " It was really the first time that AI experts worldwide\n",
    "\n",
    "02:34.240 --> 02:36.620\n",
    " were polled on this prediction.\n",
    "\n",
    "02:37.720 --> 02:39.980\n",
    " And the average poll was 100 years.\n",
    "\n",
    "02:41.420 --> 02:44.320\n",
    " 20% believed it would never happen.\n",
    "\n",
    "02:44.320 --> 02:48.120\n",
    " And that was the view in 1999.\n",
    "\n",
    "02:48.120 --> 02:50.640\n",
    " 80% believed it would happen,\n",
    "\n",
    "02:50.640 --> 02:53.200\n",
    " but not within their lifetimes.\n",
    "\n",
    "02:53.200 --> 02:55.840\n",
    " There's been so many advances in AI\n",
    "\n",
    "02:56.920 --> 03:01.840\n",
    " that the poll of AI experts has come down over the years.\n",
    "\n",
    "03:01.840 --> 03:05.440\n",
    " So a year ago, something called Meticulous,\n",
    "\n",
    "03:05.440 --> 03:07.080\n",
    " which you may be aware of,\n",
    "\n",
    "03:07.080 --> 03:11.560\n",
    " assesses different types of experts on the future.\n",
    "\n",
    "03:11.560 --> 03:16.440\n",
    " They again assessed what AI experts then felt.\n",
    "\n",
    "03:16.440 --> 03:18.940\n",
    " And they were saying 2042.\n",
    "\n",
    "03:18.940 --> 03:20.440\n",
    " For the Turing test.\n",
    "\n",
    "03:20.440 --> 03:22.440\n",
    " For the Turing test.\n",
    "\n",
    "03:22.440 --> 03:23.560\n",
    " So it's coming down.\n",
    "\n",
    "03:23.560 --> 03:26.320\n",
    " And I was still saying 2029.\n",
    "\n",
    "03:26.320 --> 03:30.200\n",
    " A few weeks ago, they again did another poll,\n",
    "\n",
    "03:30.200 --> 03:31.660\n",
    " and it was 2030.\n",
    "\n",
    "03:32.960 --> 03:37.920\n",
    " So AI experts now basically agree with me.\n",
    "\n",
    "03:37.920 --> 03:41.220\n",
    " I haven't changed at all, I've stayed with 2029.\n",
    "\n",
    "03:42.820 --> 03:44.520\n",
    " And AI experts now agree with me,\n",
    "\n",
    "03:44.520 --> 03:46.840\n",
    " but they didn't agree at first.\n",
    "\n",
    "03:46.840 --> 03:50.120\n",
    " So Alan Turing formulated the Turing test,\n",
    "\n",
    "03:50.120 --> 03:50.960\n",
    " and...\n",
    "\n",
    "03:50.960 --> 03:54.480\n",
    " Right, now, what he said was very little about it.\n",
    "\n",
    "03:54.480 --> 03:55.920\n",
    " I mean, the 1950 paper\n",
    "\n",
    "03:55.920 --> 03:58.060\n",
    " where he had articulated the Turing test,\n",
    "\n",
    "03:59.440 --> 04:04.440\n",
    " there's like a few lines that talk about the Turing test.\n",
    "\n",
    "04:06.840 --> 04:11.840\n",
    " And it really wasn't very clear how to administer it.\n",
    "\n",
    "04:12.040 --> 04:16.520\n",
    " And he said if they did it in like 15 minutes,\n",
    "\n",
    "04:16.520 --> 04:17.680\n",
    " that would be sufficient,\n",
    "\n",
    "04:17.680 --> 04:20.600\n",
    " which I don't really think is the case.\n",
    "\n",
    "04:20.600 --> 04:22.960\n",
    " These large language models now,\n",
    "\n",
    "04:22.960 --> 04:25.560\n",
    " some people are convinced by it already.\n",
    "\n",
    "04:25.560 --> 04:28.440\n",
    " I mean, you can talk to it and have a conversation with it.\n",
    "\n",
    "04:28.440 --> 04:30.340\n",
    " You can actually talk to it for hours.\n",
    "\n",
    "04:31.760 --> 04:35.360\n",
    " So it requires a little more depth.\n",
    "\n",
    "04:35.360 --> 04:38.120\n",
    " There's some problems with large language models\n",
    "\n",
    "04:38.120 --> 04:39.620\n",
    " which we can talk about.\n",
    "\n",
    "04:41.840 --> 04:46.460\n",
    " But some people are convinced by the Turing test.\n",
    "\n",
    "04:46.460 --> 04:50.160\n",
    " Now, if somebody passes the Turing test,\n",
    "\n",
    "04:50.160 --> 04:52.160\n",
    " what are the implications of that?\n",
    "\n",
    "04:52.160 --> 04:53.720\n",
    " Does that mean that they're sentient,\n",
    "\n",
    "04:53.720 --> 04:56.280\n",
    " that they're conscious or not?\n",
    "\n",
    "04:56.280 --> 05:00.880\n",
    " It's not necessarily clear what the implications are.\n",
    "\n",
    "05:00.880 --> 05:05.880\n",
    " Anyway, I believe 2029, that's six, seven years from now,\n",
    "\n",
    "05:07.640 --> 05:10.360\n",
    " we'll have something that passes the Turing test\n",
    "\n",
    "05:10.360 --> 05:12.480\n",
    " and a valid Turing test,\n",
    "\n",
    "05:12.480 --> 05:15.320\n",
    " meaning it goes for hours, not just a few minutes.\n",
    "\n",
    "05:15.320 --> 05:16.600\n",
    " Can you speak to that a little bit?\n",
    "\n",
    "05:16.600 --> 05:21.160\n",
    " What is your formulation of the Turing test?\n",
    "\n",
    "05:21.160 --> 05:23.180\n",
    " You've proposed a very difficult version\n",
    "\n",
    "05:23.180 --> 05:25.420\n",
    " of the Turing test, so what does that look like?\n",
    "\n",
    "05:25.420 --> 05:28.560\n",
    " Basically, it's just to assess it over several hours\n",
    "\n",
    "05:30.760 --> 05:35.760\n",
    " and also have a human judge that's fairly sophisticated\n",
    "\n",
    "05:36.440 --> 05:39.220\n",
    " on what computers can do and can't do.\n",
    "\n",
    "05:40.800 --> 05:43.800\n",
    " If you take somebody who's not that sophisticated\n",
    "\n",
    "05:43.800 --> 05:47.240\n",
    " or even an average engineer,\n",
    "\n",
    "05:48.360 --> 05:52.080\n",
    " they may not really assess various aspects of it.\n",
    "\n",
    "05:52.080 --> 05:55.680\n",
    " So you really want the human to challenge the system.\n",
    "\n",
    "05:55.680 --> 05:57.040\n",
    " Exactly, exactly.\n",
    "\n",
    "05:57.040 --> 05:58.520\n",
    " On its ability to do things\n",
    "\n",
    "05:58.520 --> 06:00.800\n",
    " like common sense reasoning, perhaps.\n",
    "\n",
    "06:00.800 --> 06:04.680\n",
    " That's actually a key problem with large language models.\n",
    "\n",
    "06:04.680 --> 06:08.080\n",
    " They don't do these kinds of tests\n",
    "\n",
    "06:08.080 --> 06:13.080\n",
    " that would involve assessing chains of reasoning,\n",
    "\n",
    "06:17.400 --> 06:18.960\n",
    " but you can lose track of that.\n",
    "\n",
    "06:18.960 --> 06:20.200\n",
    " If you talk to them,\n",
    "\n",
    "06:20.200 --> 06:22.760\n",
    " they actually can talk to you pretty well\n",
    "\n",
    "06:22.760 --> 06:24.840\n",
    " and you can be convinced by it,\n",
    "\n",
    "06:24.840 --> 06:27.400\n",
    " but it's somebody that would really convince you\n",
    "\n",
    "06:27.400 --> 06:32.200\n",
    " that it's a human, whatever that takes.\n",
    "\n",
    "06:32.200 --> 06:34.800\n",
    " Maybe it would take days or weeks,\n",
    "\n",
    "06:34.800 --> 06:38.720\n",
    " but it would really convince you that it's human.\n",
    "\n",
    "06:40.880 --> 06:45.320\n",
    " Large language models can appear that way.\n",
    "\n",
    "06:45.320 --> 06:49.760\n",
    " You can read conversations and they appear pretty good.\n",
    "\n",
    "06:49.760 --> 06:52.260\n",
    " There are some problems with it.\n",
    "\n",
    "06:52.260 --> 06:54.140\n",
    " It doesn't do math very well.\n",
    "\n",
    "06:55.000 --> 06:58.160\n",
    " You can ask how many legs did 10 elephants have\n",
    "\n",
    "06:58.160 --> 07:00.020\n",
    " and they'll tell you, well, okay,\n",
    "\n",
    "07:00.020 --> 07:01.440\n",
    " each elephant has four legs\n",
    "\n",
    "07:01.440 --> 07:03.700\n",
    " and it's 10 elephants, so it's 40 legs.\n",
    "\n",
    "07:03.700 --> 07:05.840\n",
    " And you go, okay, that's pretty good.\n",
    "\n",
    "07:05.840 --> 07:07.960\n",
    " How many legs do 11 elephants have?\n",
    "\n",
    "07:07.960 --> 07:11.520\n",
    " And they don't seem to understand the question.\n",
    "\n",
    "07:11.520 --> 07:14.160\n",
    " Do all humans understand that question?\n",
    "\n",
    "07:14.160 --> 07:15.880\n",
    " No, that's the key thing.\n",
    "\n",
    "07:15.880 --> 07:19.440\n",
    " I mean, how advanced a human do you want it to be?\n",
    "\n",
    "07:19.440 --> 07:21.360\n",
    " But we do expect a human\n",
    "\n",
    "07:21.360 --> 07:23.980\n",
    " to be able to do multi chain reasoning,\n",
    "\n",
    "07:24.840 --> 07:26.320\n",
    " to be able to take a few facts\n",
    "\n",
    "07:26.320 --> 07:29.840\n",
    " and put them together, not perfectly.\n",
    "\n",
    "07:29.840 --> 07:32.800\n",
    " And we see that in a lot of polls\n",
    "\n",
    "07:32.800 --> 07:35.540\n",
    " that people don't do that perfectly at all.\n",
    "\n",
    "07:39.220 --> 07:42.020\n",
    " So it's not very well defined,\n",
    "\n",
    "07:42.020 --> 07:44.320\n",
    " but it's something where it really would convince you\n",
    "\n",
    "07:44.320 --> 07:45.600\n",
    " that it's a human.\n",
    "\n",
    "07:45.600 --> 07:48.840\n",
    " Is your intuition that large language models\n",
    "\n",
    "07:48.840 --> 07:52.320\n",
    " will not be solely the kind of system\n",
    "\n",
    "07:52.320 --> 07:55.600\n",
    " that passes the Turing test in 2029?\n",
    "\n",
    "07:55.600 --> 07:56.800\n",
    " Do we need something else?\n",
    "\n",
    "07:56.800 --> 07:58.720\n",
    " No, I think it will be a large language model,\n",
    "\n",
    "07:58.720 --> 08:02.960\n",
    " but they have to go beyond what they're doing now.\n",
    "\n",
    "08:02.960 --> 08:04.400\n",
    " I think we're getting there.\n",
    "\n",
    "08:05.760 --> 08:09.240\n",
    " And another key issue is if somebody\n",
    "\n",
    "08:09.240 --> 08:12.200\n",
    " actually passes the Turing test validly,\n",
    "\n",
    "08:12.200 --> 08:13.640\n",
    " I would believe they're conscious.\n",
    "\n",
    "08:13.640 --> 08:15.000\n",
    " And then not everybody would say that.\n",
    "\n",
    "08:15.000 --> 08:17.440\n",
    " It's okay, we can pass the Turing test,\n",
    "\n",
    "08:17.440 --> 08:20.080\n",
    " but we don't really believe that it's conscious.\n",
    "\n",
    "08:20.080 --> 08:21.480\n",
    " That's a whole nother issue.\n",
    "\n",
    "08:23.120 --> 08:24.920\n",
    " But if it really passes the Turing test,\n",
    "\n",
    "08:24.920 --> 08:26.720\n",
    " I would believe that it's conscious.\n",
    "\n",
    "08:26.720 --> 08:31.720\n",
    " But I don't believe that of large language models today.\n",
    "\n",
    "08:32.760 --> 08:35.520\n",
    " If it appears to be conscious,\n",
    "\n",
    "08:35.520 --> 08:38.240\n",
    " that's as good as being conscious, at least for you,\n",
    "\n",
    "08:38.240 --> 08:40.700\n",
    " in some sense.\n",
    "\n",
    "08:40.700 --> 08:45.300\n",
    " I mean, consciousness is not something that's scientific.\n",
    "\n",
    "08:46.640 --> 08:48.880\n",
    " I mean, I believe you're conscious,\n",
    "\n",
    "08:49.760 --> 08:51.100\n",
    " but it's really just a belief,\n",
    "\n",
    "08:51.100 --> 08:52.800\n",
    " and we believe that about other humans\n",
    "\n",
    "08:52.800 --> 08:57.400\n",
    " that at least appear to be conscious.\n",
    "\n",
    "08:57.400 --> 09:00.460\n",
    " When you go outside of shared human assumption,\n",
    "\n",
    "09:01.720 --> 09:03.640\n",
    " like are animals conscious?\n",
    "\n",
    "09:04.520 --> 09:06.200\n",
    " Some people believe they're not conscious.\n",
    "\n",
    "09:06.200 --> 09:08.680\n",
    " Some people believe they are conscious.\n",
    "\n",
    "09:08.680 --> 09:13.680\n",
    " And would a machine that acts just like a human be conscious?\n",
    "\n",
    "09:14.520 --> 09:16.200\n",
    " I mean, I believe it would be.\n",
    "\n",
    "09:17.040 --> 09:20.800\n",
    " But that's really a philosophical belief.\n",
    "\n",
    "09:20.800 --> 09:22.720\n",
    " You can't prove it.\n",
    "\n",
    "09:22.720 --> 09:25.480\n",
    " I can't take an entity and prove that it's conscious.\n",
    "\n",
    "09:25.480 --> 09:27.280\n",
    " There's nothing that you can do\n",
    "\n",
    "09:27.280 --> 09:30.360\n",
    " that would indicate that.\n",
    "\n",
    "09:30.360 --> 09:32.780\n",
    " It's like saying a piece of art is beautiful.\n",
    "\n",
    "09:32.780 --> 09:35.000\n",
    " You can say it.\n",
    "\n",
    "09:35.000 --> 09:38.200\n",
    " Multiple people can experience a piece of art as beautiful,\n",
    "\n",
    "09:39.300 --> 09:41.320\n",
    " but you can't prove it.\n",
    "\n",
    "09:41.320 --> 09:44.840\n",
    " But it's also an extremely important issue.\n",
    "\n",
    "09:44.840 --> 09:47.040\n",
    " I mean, imagine if you had something\n",
    "\n",
    "09:47.040 --> 09:49.140\n",
    " where nobody's conscious.\n",
    "\n",
    "09:49.140 --> 09:52.660\n",
    " The world may as well not exist.\n",
    "\n",
    "09:55.660 --> 10:00.060\n",
    " And so some people, like say Marvin Minsky,\n",
    "\n",
    "10:02.620 --> 10:05.940\n",
    " said, well, consciousness is not logical,\n",
    "\n",
    "10:05.940 --> 10:08.380\n",
    " it's not scientific, and therefore we should dismiss it,\n",
    "\n",
    "10:08.380 --> 10:13.380\n",
    " and any talk about consciousness is just not to be believed.\n",
    "\n",
    "12:51.160 --> 12:52.920\n",
    " You could actually take it back in time.\n",
    "\n",
    "12:52.920 --> 12:55.840\n",
    " You could eliminate its memory and have it go over again.\n",
    "\n",
    "12:55.840 --> 12:59.800\n",
    " I mean, it has a different kind of connotation\n",
    "\n",
    "12:59.800 --> 13:01.800\n",
    " than humans do.\n",
    "\n",
    "13:01.800 --> 13:04.400\n",
    " Well, perhaps it can do the same thing with humans.\n",
    "\n",
    "13:04.400 --> 13:06.880\n",
    " It's just that we don't know how to do that yet.\n",
    "\n",
    "13:06.880 --> 13:09.400\n",
    " It's possible that we figure out all of these things\n",
    "\n",
    "13:09.400 --> 13:10.780\n",
    " on the machine first.\n",
    "\n",
    "13:12.320 --> 13:15.480\n",
    " But that doesn't mean the machine isn't conscious.\n",
    "\n",
    "13:15.480 --> 13:17.640\n",
    " I mean, if you look at the way people react,\n",
    "\n",
    "13:17.640 --> 13:22.640\n",
    " say, 3CPO or other machines that are conscious in movies,\n",
    "\n",
    "13:25.000 --> 13:26.740\n",
    " they don't actually present how it's conscious,\n",
    "\n",
    "13:26.740 --> 13:30.120\n",
    " but we see that they are a machine\n",
    "\n",
    "13:30.120 --> 13:33.280\n",
    " and people will believe that they are conscious\n",
    "\n",
    "13:33.280 --> 13:34.640\n",
    " and they'll actually worry about it\n",
    "\n",
    "13:34.640 --> 13:37.480\n",
    " if they get into trouble and so on.\n",
    "\n",
    "13:37.480 --> 13:40.840\n",
    " So 2029 is going to be the first year\n",
    "\n",
    "13:40.840 --> 13:43.440\n",
    " when a major thing happens.\n",
    "\n",
    "13:43.440 --> 13:44.280\n",
    " Right.\n",
    "\n",
    "13:44.280 --> 13:46.520\n",
    " And that will shake our civilization\n",
    "\n",
    "13:46.520 --> 13:50.280\n",
    " to start to consider the role of AI in this world.\n",
    "\n",
    "13:50.280 --> 13:51.120\n",
    " Yes and no.\n",
    "\n",
    "13:51.120 --> 13:54.560\n",
    " I mean, this one guy at Google claimed\n",
    "\n",
    "13:54.560 --> 13:58.440\n",
    " that the machine was conscious.\n",
    "\n",
    "13:58.440 --> 14:00.160\n",
    " But that's just one person.\n",
    "\n",
    "14:00.160 --> 14:01.000\n",
    " Right.\n",
    "\n",
    "14:01.000 --> 14:03.080\n",
    " When it starts to happen to scale.\n",
    "\n",
    "14:03.080 --> 14:06.320\n",
    " Well, that's exactly right because most people\n",
    "\n",
    "14:06.320 --> 14:07.760\n",
    " have not taken that position.\n",
    "\n",
    "14:07.760 --> 14:08.940\n",
    " I don't take that position.\n",
    "\n",
    "14:08.940 --> 14:13.940\n",
    " I mean, I've used different things like this\n",
    "\n",
    "14:17.240 --> 14:20.500\n",
    " and they don't appear to me to be conscious.\n",
    "\n",
    "14:20.500 --> 14:22.840\n",
    " As we eliminate various problems\n",
    "\n",
    "14:22.840 --> 14:25.840\n",
    " of these large language models,\n",
    "\n",
    "14:26.960 --> 14:30.480\n",
    " more and more people will accept that they're conscious.\n",
    "\n",
    "14:30.480 --> 14:35.480\n",
    " So when we get to 2029, I think a large fraction\n",
    "\n",
    "14:35.760 --> 14:37.960\n",
    " of people will believe that they're conscious.\n",
    "\n",
    "14:39.080 --> 14:41.040\n",
    " So it's not gonna happen all at once.\n",
    "\n",
    "14:42.440 --> 14:44.360\n",
    " I believe it will actually happen gradually\n",
    "\n",
    "14:44.360 --> 14:46.240\n",
    " and it's already started to happen.\n",
    "\n",
    "14:47.280 --> 14:52.280\n",
    " And so that takes us one step closer to the singularity.\n",
    "\n",
    "14:52.360 --> 14:55.560\n",
    " Another step then is in the 2030s\n",
    "\n",
    "14:55.560 --> 14:59.800\n",
    " when we can actually connect our neocortex,\n",
    "\n",
    "14:59.800 --> 15:04.800\n",
    " which is where we do our thinking, to computers.\n",
    "\n",
    "15:04.880 --> 15:09.280\n",
    " And I mean, just as this actually gains a lot\n",
    "\n",
    "15:09.280 --> 15:12.200\n",
    " to being connected to computers\n",
    "\n",
    "15:12.200 --> 15:15.360\n",
    " that will amplify its abilities,\n",
    "\n",
    "15:15.360 --> 15:17.400\n",
    " I mean, if this did not have any connection,\n",
    "\n",
    "15:17.400 --> 15:19.360\n",
    " it would be pretty stupid.\n",
    "\n",
    "15:19.360 --> 15:21.860\n",
    " It could not answer any of your questions.\n",
    "\n",
    "15:21.860 --> 15:24.400\n",
    " If you're just listening to this, by the way,\n",
    "\n",
    "15:24.400 --> 15:29.400\n",
    " Ray's holding up the all powerful smartphone.\n",
    "\n",
    "15:29.400 --> 15:32.480\n",
    " So we're gonna do that directly from our brains.\n",
    "\n",
    "15:33.520 --> 15:35.040\n",
    " I mean, these are pretty good.\n",
    "\n",
    "15:35.040 --> 15:37.720\n",
    " These already have amplified our intelligence.\n",
    "\n",
    "15:37.720 --> 15:40.040\n",
    " I'm already much smarter than I would otherwise be\n",
    "\n",
    "15:40.040 --> 15:41.480\n",
    " if I didn't have this.\n",
    "\n",
    "15:42.600 --> 15:44.240\n",
    " Because I remember my first book,\n",
    "\n",
    "15:44.240 --> 15:45.920\n",
    " The Age of Intelligent Machines,\n",
    "\n",
    "15:49.060 --> 15:52.080\n",
    " there was no way to get information from computers.\n",
    "\n",
    "15:52.080 --> 15:55.400\n",
    " I actually would go to a library, find a book,\n",
    "\n",
    "15:55.400 --> 15:58.440\n",
    " find the page that had an information I wanted,\n",
    "\n",
    "15:58.440 --> 15:59.920\n",
    " and I'd go to the copier,\n",
    "\n",
    "15:59.920 --> 16:04.360\n",
    " and my most significant information tool\n",
    "\n",
    "16:04.360 --> 16:08.480\n",
    " was a roll of quarters where I could feed the copier.\n",
    "\n",
    "16:08.480 --> 16:11.400\n",
    " So we're already greatly advanced\n",
    "\n",
    "16:11.400 --> 16:13.280\n",
    " that we have these things.\n",
    "\n",
    "16:13.280 --> 16:15.460\n",
    " There's a few problems with it.\n",
    "\n",
    "16:15.460 --> 16:17.280\n",
    " First of all, I constantly put it down,\n",
    "\n",
    "16:17.280 --> 16:19.680\n",
    " and I don't remember where I put it.\n",
    "\n",
    "16:19.680 --> 16:21.220\n",
    " I've actually never lost it.\n",
    "\n",
    "16:21.220 --> 16:26.080\n",
    " But you have to find it, and then you have to turn it on.\n",
    "\n",
    "16:26.080 --> 16:28.160\n",
    " So there's a certain amount of steps.\n",
    "\n",
    "16:28.160 --> 16:30.100\n",
    " It would actually be quite useful\n",
    "\n",
    "16:30.100 --> 16:33.440\n",
    " if someone would just listen to your conversation\n",
    "\n",
    "16:33.440 --> 16:38.440\n",
    " and say, oh, that's so and so actress,\n",
    "\n",
    "16:38.920 --> 16:41.160\n",
    " and tell you what you're talking about.\n",
    "\n",
    "16:41.160 --> 16:43.160\n",
    " So going from active to passive,\n",
    "\n",
    "16:43.160 --> 16:46.240\n",
    " where it just permeates your whole life.\n",
    "\n",
    "16:46.240 --> 16:47.280\n",
    " Yeah, exactly.\n",
    "\n",
    "16:47.280 --> 16:49.560\n",
    " The way your brain does when you're awake.\n",
    "\n",
    "16:49.560 --> 16:51.220\n",
    " Your brain is always there.\n",
    "\n",
    "16:51.220 --> 16:52.060\n",
    " Right.\n",
    "\n",
    "16:52.060 --> 16:53.800\n",
    " That's something that could actually\n",
    "\n",
    "16:53.800 --> 16:55.840\n",
    " just about be done today,\n",
    "\n",
    "16:55.840 --> 16:57.400\n",
    " where we'd listen to your conversation,\n",
    "\n",
    "16:57.400 --> 16:58.600\n",
    " understand what you're saying,\n",
    "\n",
    "16:58.600 --> 17:01.840\n",
    " understand what you're not missing,\n",
    "\n",
    "17:01.840 --> 17:03.600\n",
    " and give you that information.\n",
    "\n",
    "17:04.520 --> 17:07.300\n",
    " But another step is to actually go inside your brain.\n",
    "\n",
    "17:09.720 --> 17:12.740\n",
    " And there are some prototypes\n",
    "\n",
    "17:12.740 --> 17:15.280\n",
    " where you can connect your brain.\n",
    "\n",
    "17:15.280 --> 17:17.040\n",
    " They actually don't have the amount\n",
    "\n",
    "17:17.040 --> 17:19.160\n",
    " of bandwidth that we need.\n",
    "\n",
    "17:19.160 --> 17:21.940\n",
    " They can work, but they work fairly slowly.\n",
    "\n",
    "17:21.940 --> 17:26.160\n",
    " So if it actually would connect to your neocortex,\n",
    "\n",
    "17:26.160 --> 17:30.180\n",
    " and the neocortex, which I describe\n",
    "\n",
    "17:30.180 --> 17:31.740\n",
    " in How to Create a Mind,\n",
    "\n",
    "17:33.000 --> 17:34.820\n",
    " the neocortex is actually,\n",
    "\n",
    "17:36.700 --> 17:38.180\n",
    " it has different levels,\n",
    "\n",
    "17:38.180 --> 17:39.980\n",
    " and as you go up the levels,\n",
    "\n",
    "17:39.980 --> 17:41.780\n",
    " it's kind of like a pyramid.\n",
    "\n",
    "17:41.780 --> 17:44.340\n",
    " The top level is fairly small,\n",
    "\n",
    "17:44.340 --> 17:46.540\n",
    " and that's the level where you wanna connect\n",
    "\n",
    "17:47.820 --> 17:50.140\n",
    " these brain extenders.\n",
    "\n",
    "17:50.140 --> 17:55.140\n",
    " And so I believe that will happen in the 2030s.\n",
    "\n",
    "17:58.100 --> 18:01.580\n",
    " So just the way this is greatly amplified\n",
    "\n",
    "18:01.580 --> 18:03.480\n",
    " by being connected to the cloud,\n",
    "\n",
    "18:04.420 --> 18:07.420\n",
    " we can connect our own brain to the cloud,\n",
    "\n",
    "18:07.420 --> 18:12.420\n",
    " and just do what we can do by using this machine.\n",
    "\n",
    "18:14.260 --> 18:15.660\n",
    " Do you think it would look like\n",
    "\n",
    "18:15.660 --> 18:18.920\n",
    " the brain computer interface of like Neuralink?\n",
    "\n",
    "18:18.920 --> 18:19.760\n",
    " So would it be?\n",
    "\n",
    "18:19.760 --> 18:22.500\n",
    " Well, Neuralink, it's an attempt to do that.\n",
    "\n",
    "18:22.500 --> 18:24.920\n",
    " It doesn't have the bandwidth that we need.\n",
    "\n",
    "18:26.300 --> 18:27.660\n",
    " Yet, right?\n",
    "\n",
    "18:27.660 --> 18:29.240\n",
    " Right, but I think,\n",
    "\n",
    "18:30.320 --> 18:31.980\n",
    " I mean, they're gonna get permission for this\n",
    "\n",
    "18:31.980 --> 18:33.160\n",
    " because there are a lot of people\n",
    "\n",
    "18:33.160 --> 18:36.660\n",
    " who absolutely need it because they can't communicate.\n",
    "\n",
    "18:36.660 --> 18:38.420\n",
    " I know a couple people like that\n",
    "\n",
    "18:38.420 --> 18:41.220\n",
    " who have ideas and they cannot,\n",
    "\n",
    "18:42.660 --> 18:44.580\n",
    " they cannot move their muscles and so on.\n",
    "\n",
    "18:44.580 --> 18:45.800\n",
    " They can't communicate.\n",
    "\n",
    "18:45.800 --> 18:50.800\n",
    " And so for them, this would be very valuable,\n",
    "\n",
    "18:52.040 --> 18:53.320\n",
    " but we could all use it.\n",
    "\n",
    "18:54.820 --> 18:56.600\n",
    " Basically, it'd be,\n",
    "\n",
    "18:59.000 --> 19:02.520\n",
    " turn us into something that would be like we have a phone,\n",
    "\n",
    "19:02.520 --> 19:05.120\n",
    " but it would be in our minds.\n",
    "\n",
    "19:05.120 --> 19:07.360\n",
    " It would be kind of instantaneous.\n",
    "\n",
    "19:07.360 --> 19:09.440\n",
    " And maybe communication between two people\n",
    "\n",
    "19:09.440 --> 19:14.080\n",
    " would not require this low bandwidth mechanism of language.\n",
    "\n",
    "19:14.080 --> 19:15.640\n",
    " Yes, exactly.\n",
    "\n",
    "19:15.640 --> 19:17.280\n",
    " We don't know what that would be,\n",
    "\n",
    "19:17.280 --> 19:22.280\n",
    " although we do know that computers can share information\n",
    "\n",
    "19:22.400 --> 19:24.640\n",
    " like language instantly.\n",
    "\n",
    "19:24.640 --> 19:28.880\n",
    " They can share many, many books in a second.\n",
    "\n",
    "19:28.880 --> 19:31.200\n",
    " So we could do that as well.\n",
    "\n",
    "19:31.200 --> 19:34.240\n",
    " If you look at what our brain does,\n",
    "\n",
    "19:34.240 --> 19:39.120\n",
    " it actually can manipulate different parameters.\n",
    "\n",
    "19:39.120 --> 19:44.120\n",
    " So we talk about these large language models.\n",
    "\n",
    "19:46.560 --> 19:48.320\n",
    " I mean, I had written that\n",
    "\n",
    "19:51.520 --> 19:55.000\n",
    " it requires a certain amount of information\n",
    "\n",
    "19:55.000 --> 19:57.520\n",
    " in order to be effective\n",
    "\n",
    "19:58.600 --> 20:01.920\n",
    " and that we would not see AI really being effective\n",
    "\n",
    "20:01.920 --> 20:03.480\n",
    " until it got to that level.\n",
    "\n",
    "20:04.400 --> 20:06.400\n",
    " And we had large language models\n",
    "\n",
    "20:06.400 --> 20:09.600\n",
    " that were like 10 billion bytes, didn't work very well.\n",
    "\n",
    "20:09.600 --> 20:11.680\n",
    " They finally got to a hundred billion bytes\n",
    "\n",
    "20:11.680 --> 20:13.440\n",
    " and now they work fairly well.\n",
    "\n",
    "20:13.440 --> 20:16.280\n",
    " And now we're going to a trillion bytes.\n",
    "\n",
    "20:16.280 --> 20:21.280\n",
    " If you say lambda has a hundred billion bytes,\n",
    "\n",
    "20:22.520 --> 20:23.520\n",
    " what does that mean?\n",
    "\n",
    "20:23.520 --> 20:27.160\n",
    " Well, what if you had something that had one byte,\n",
    "\n",
    "20:27.160 --> 20:30.000\n",
    " one parameter, maybe you wanna tell\n",
    "\n",
    "20:30.000 --> 20:33.960\n",
    " whether or not something's an elephant or not.\n",
    "\n",
    "20:33.960 --> 20:37.680\n",
    " And so you put in something that would detect its trunk.\n",
    "\n",
    "1:05:46.200 --> 1:05:50.500\n",
    " I didn't, I wasn't inherently a futurist.\n",
    "\n",
    "1:05:50.500 --> 1:05:52.620\n",
    " That was not really my goal.\n",
    "\n",
    "1:05:54.320 --> 1:05:57.400\n",
    " It's really to figure out when things are feasible.\n",
    "\n",
    "1:05:57.400 --> 1:06:00.800\n",
    " We see that now with large scale models.\n",
    "\n",
    "1:06:01.680 --> 1:06:06.400\n",
    " The very large scale models like GPT3,\n",
    "\n",
    "1:06:06.400 --> 1:06:08.200\n",
    " it started two years ago.\n",
    "\n",
    "1:06:09.600 --> 1:06:11.160\n",
    " Four years ago, it wasn't feasible.\n",
    "\n",
    "1:06:11.160 --> 1:06:16.160\n",
    " In fact, they did create GPT2, which didn't work.\n",
    "\n",
    "1:06:18.800 --> 1:06:22.360\n",
    " So it required a certain amount of timing\n",
    "\n",
    "1:06:22.360 --> 1:06:24.200\n",
    " having to do with this exponential growth\n",
    "\n",
    "1:06:24.200 --> 1:06:27.400\n",
    " of computing power.\n",
    "\n",
    "1:06:27.400 --> 1:06:31.240\n",
    " So futurism in some sense is a study of timing,\n",
    "\n",
    "1:06:31.240 --> 1:06:34.400\n",
    " trying to understand how the world will evolve\n",
    "\n",
    "1:06:34.400 --> 1:06:38.320\n",
    " and when will the capacity for certain ideas emerge.\n",
    "\n",
    "1:06:38.320 --> 1:06:40.040\n",
    " And that's become a thing in itself\n",
    "\n",
    "1:06:40.040 --> 1:06:42.560\n",
    " and to try to time things in the future.\n",
    "\n",
    "1:06:43.960 --> 1:06:48.960\n",
    " But really its original purpose was to time my products.\n",
    "\n",
    "1:06:48.960 --> 1:06:53.800\n",
    " I mean, I did OCR in the 1970s\n",
    "\n",
    "1:06:55.480 --> 1:07:00.480\n",
    " because OCR doesn't require a lot of computation.\n",
    "\n",
    "1:07:01.440 --> 1:07:02.760\n",
    " Optical character recognition.\n",
    "\n",
    "1:07:02.760 --> 1:07:06.560\n",
    " Yeah, so we were able to do that in the 70s\n",
    "\n",
    "1:07:06.560 --> 1:07:11.000\n",
    " and I waited till the 80s to address speech recognition\n",
    "\n",
    "1:07:11.000 --> 1:07:14.480\n",
    " since that requires more computation.\n",
    "\n",
    "1:07:14.480 --> 1:07:16.000\n",
    " So you were thinking through timing\n",
    "\n",
    "1:07:16.000 --> 1:07:17.480\n",
    " when you're developing those things.\n",
    "\n",
    "1:07:17.480 --> 1:07:18.320\n",
    " Yeah.\n",
    "\n",
    "1:07:18.320 --> 1:07:19.880\n",
    " Time come.\n",
    "\n",
    "1:07:19.880 --> 1:07:21.400\n",
    " Yeah.\n",
    "\n",
    "1:07:21.400 --> 1:07:24.360\n",
    " And that's how you've developed that brain power\n",
    "\n",
    "1:07:24.360 --> 1:07:26.720\n",
    " to start to think in a futurist sense\n",
    "\n",
    "1:07:26.720 --> 1:07:31.040\n",
    " when how will the world look like in 2045\n",
    "\n",
    "1:35:58.460 --> 1:36:21.460\n",
    " Thank you for listening, and hope to see you next time.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "abab81a7-dc55-4a62-9bb5-436384b7eab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WEBVTT By the time he gets to 2045, This is the Lex Friedman podcast.\n",
      " To support it, please check out our sponsors in the description.\n",
      " And now, dear friends, here's Ray Kurzweil.\n",
      " In your 2005 book titled The Singularity is Near, you predicted that the singularity will happen in 2045.\n",
      " So now, 18 years later, do you still estimate that the singularity will happen on 2045? And maybe first, what is the singularity, the technological singularity, and when will it happen? Singularity is where computers really change our view of what's important and change who we are.\n",
      " But we're getting close to some salient things that will change who we are.\n",
      " A key thing is 2029, when computers will pass the Turing test.\n",
      " And there's also some controversy whether the Turing test is valid.\n",
      " I believe it is.\n",
      " Most people do believe that, but there's some controversy about that.\n",
      " But Stanford got very alarmed at my prediction about 2029.\n",
      " I made this in 1999 in my book.\n",
      " The Age of Spiritual Machines.\n",
      " Right.\n",
      " And then you repeated the prediction in 2005.\n",
      " In 2005.\n",
      " Yeah.\n",
      " So they held an international conference, you might have been aware of it, of AI experts in 1999 to assess this view.\n",
      " So people gave different predictions, and they took a poll.\n",
      " It was really the first time that AI experts worldwide were polled on this prediction.\n",
      " And the average poll was 100 years.\n",
      " 20% believed it would never happen.\n",
      " And that was the view in 1999.\n",
      " 80% believed it would happen, but not within their lifetimes.\n",
      " There's been so many advances in AI that the poll of AI experts has come down over the years.\n",
      " So a year ago, something called Meticulous, which you may be aware of, assesses different types of experts on the future.\n",
      " They again assessed what AI experts then felt.\n",
      " And they were saying 2042.\n",
      " For the Turing test.\n",
      " For the Turing test.\n",
      " So it's coming down.\n",
      " And I was still saying 2029.\n",
      " A few weeks ago, they again did another poll, and it was 2030.\n",
      " So AI experts now basically agree with me.\n",
      " I haven't changed at all, I've stayed with 2029.\n",
      " And AI experts now agree with me, but they didn't agree at first.\n",
      " So Alan Turing formulated the Turing test, and...\n",
      " Right, now, what he said was very little about it.\n",
      " I mean, the 1950 paper where he had articulated the Turing test, there's like a few lines that talk about the Turing test.\n",
      " And it really wasn't very clear how to administer it.\n",
      " And he said if they did it in like 15 minutes, that would be sufficient, which I don't really think is the case.\n",
      " These large language models now, some people are convinced by it already.\n",
      " I mean, you can talk to it and have a conversation with it.\n",
      " You can actually talk to it for hours.\n",
      " So it requires a little more depth.\n",
      " There's some problems with large language models which we can talk about.\n",
      " But some people are convinced by the Turing test.\n",
      " Now, if somebody passes the Turing test, what are the implications of that? Does that mean that they're sentient, that they're conscious or not? It's not necessarily clear what the implications are.\n",
      " Anyway, I believe 2029, that's six, seven years from now, we'll have something that passes the Turing test and a valid Turing test, meaning it goes for hours, not just a few minutes.\n",
      " Can you speak to that a little bit? What is your formulation of the Turing test? You've proposed a very difficult version of the Turing test, so what does that look like? Basically, it's just to assess it over several hours and also have a human judge that's fairly sophisticated on what computers can do and can't do.\n",
      " If you take somebody who's not that sophisticated or even an average engineer, they may not really assess various aspects of it.\n",
      " So you really want the human to challenge the system.\n",
      " Exactly, exactly.\n",
      " On its ability to do things like common sense reasoning, perhaps.\n",
      " That's actually a key problem with large language models.\n",
      " They don't do these kinds of tests that would involve assessing chains of reasoning, but you can lose track of that.\n",
      " If you talk to them, they actually can talk to you pretty well and you can be convinced by it, but it's somebody that would really convince you that it's a human, whatever that takes.\n",
      " Maybe it would take days or weeks, but it would really convince you that it's human.\n",
      " Large language models can appear that way.\n",
      " You can read conversations and they appear pretty good.\n",
      " There are some problems with it.\n",
      " It doesn't do math very well.\n",
      " You can ask how many legs did 10 elephants have and they'll tell you, well, okay, each elephant has four legs and it's 10 elephants, so it's 40 legs.\n",
      " And you go, okay, that's pretty good.\n",
      " How many legs do 11 elephants have? And they don't seem to understand the question.\n",
      " Do all humans understand that question? No, that's the key thing.\n",
      " I mean, how advanced a human do you want it to be? But we do expect a human to be able to do multi chain reasoning, to be able to take a few facts and put them together, not perfectly.\n",
      " And we see that in a lot of polls that people don't do that perfectly at all.\n",
      " So it's not very well defined, but it's something where it really would convince you that it's a human.\n",
      " Is your intuition that large language models will not be solely the kind of system that passes the Turing test in 2029? Do we need something else? No, I think it will be a large language model, but they have to go beyond what they're doing now.\n",
      " I think we're getting there.\n",
      " And another key issue is if somebody actually passes the Turing test validly, I would believe they're conscious.\n",
      " And then not everybody would say that.\n",
      " It's okay, we can pass the Turing test, but we don't really believe that it's conscious.\n",
      " That's a whole nother issue.\n",
      " But if it really passes the Turing test, I would believe that it's conscious.\n",
      " But I don't believe that of large language models today.\n",
      " If it appears to be conscious, that's as good as being conscious, at least for you, in some sense.\n",
      " I mean, consciousness is not something that's scientific.\n",
      " I mean, I believe you're conscious, but it's really just a belief, and we believe that about other humans that at least appear to be conscious.\n",
      " When you go outside of shared human assumption, like are animals conscious? Some people believe they're not conscious.\n",
      " Some people believe they are conscious.\n",
      " And would a machine that acts just like a human be conscious? I mean, I believe it would be.\n",
      " But that's really a philosophical belief.\n",
      " You can't prove it.\n",
      " I can't take an entity and prove that it's conscious.\n",
      " There's nothing that you can do that would indicate that.\n",
      " It's like saying a piece of art is beautiful.\n",
      " You can say it.\n",
      " Multiple people can experience a piece of art as beautiful, but you can't prove it.\n",
      " But it's also an extremely important issue.\n",
      " I mean, imagine if you had something where nobody's conscious.\n",
      " The world may as well not exist.\n",
      " And so some people, like say Marvin Minsky, said, well, consciousness is not logical, it's not scientific, and therefore we should dismiss it, and any talk about consciousness is just not to be believed.\n",
      " You could actually take it back in time.\n",
      " You could eliminate its memory and have it go over again.\n",
      " I mean, it has a different kind of connotation than humans do.\n",
      " Well, perhaps it can do the same thing with humans.\n",
      " It's just that we don't know how to do that yet.\n",
      " It's possible that we figure out all of these things on the machine first.\n",
      " But that doesn't mean the machine isn't conscious.\n",
      " I mean, if you look at the way people react, say, 3CPO or other machines that are conscious in movies, they don't actually present how it's conscious, but we see that they are a machine and people will believe that they are conscious and they'll actually worry about it if they get into trouble and so on.\n",
      " So 2029 is going to be the first year when a major thing happens.\n",
      " Right.\n",
      " And that will shake our civilization to start to consider the role of AI in this world.\n",
      " Yes and no.\n",
      " I mean, this one guy at Google claimed that the machine was conscious.\n",
      " But that's just one person.\n",
      " Right.\n",
      " When it starts to happen to scale.\n",
      " Well, that's exactly right because most people have not taken that position.\n",
      " I don't take that position.\n",
      " I mean, I've used different things like this and they don't appear to me to be conscious.\n",
      " As we eliminate various problems of these large language models, more and more people will accept that they're conscious.\n",
      " So when we get to 2029, I think a large fraction of people will believe that they're conscious.\n",
      " So it's not gonna happen all at once.\n",
      " I believe it will actually happen gradually and it's already started to happen.\n",
      " And so that takes us one step closer to the singularity.\n",
      " Another step then is in the 2030s when we can actually connect our neocortex, which is where we do our thinking, to computers.\n",
      " And I mean, just as this actually gains a lot to being connected to computers that will amplify its abilities, I mean, if this did not have any connection, it would be pretty stupid.\n",
      " It could not answer any of your questions.\n",
      " If you're just listening to this, by the way, Ray's holding up the all powerful smartphone.\n",
      " So we're gonna do that directly from our brains.\n",
      " I mean, these are pretty good.\n",
      " These already have amplified our intelligence.\n",
      " I'm already much smarter than I would otherwise be if I didn't have this.\n",
      " Because I remember my first book, The Age of Intelligent Machines, there was no way to get information from computers.\n",
      " I actually would go to a library, find a book, find the page that had an information I wanted, and I'd go to the copier, and my most significant information tool was a roll of quarters where I could feed the copier.\n",
      " So we're already greatly advanced that we have these things.\n",
      " There's a few problems with it.\n",
      " First of all, I constantly put it down, and I don't remember where I put it.\n",
      " I've actually never lost it.\n",
      " But you have to find it, and then you have to turn it on.\n",
      " So there's a certain amount of steps.\n",
      " It would actually be quite useful if someone would just listen to your conversation and say, oh, that's so and so actress, and tell you what you're talking about.\n",
      " So going from active to passive, where it just permeates your whole life.\n",
      " Yeah, exactly.\n",
      " The way your brain does when you're awake.\n",
      " Your brain is always there.\n",
      " Right.\n",
      " That's something that could actually just about be done today, where we'd listen to your conversation, understand what you're saying, understand what you're not missing, and give you that information.\n",
      " But another step is to actually go inside your brain.\n",
      " And there are some prototypes where you can connect your brain.\n",
      " They actually don't have the amount of bandwidth that we need.\n",
      " They can work, but they work fairly slowly.\n",
      " So if it actually would connect to your neocortex, and the neocortex, which I describe in How to Create a Mind, the neocortex is actually, it has different levels, and as you go up the levels, it's kind of like a pyramid.\n",
      " The top level is fairly small, and that's the level where you wanna connect these brain extenders.\n",
      " And so I believe that will happen in the 2030s.\n",
      " So just the way this is greatly amplified by being connected to the cloud, we can connect our own brain to the cloud, and just do what we can do by using this machine.\n",
      " Do you think it would look like the brain computer interface of like Neuralink? So would it be? Well, Neuralink, it's an attempt to do that.\n",
      " It doesn't have the bandwidth that we need.\n",
      " Yet, right? Right, but I think, I mean, they're gonna get permission for this because there are a lot of people who absolutely need it because they can't communicate.\n",
      " I know a couple people like that who have ideas and they cannot, they cannot move their muscles and so on.\n",
      " They can't communicate.\n",
      " And so for them, this would be very valuable, but we could all use it.\n",
      " Basically, it'd be, turn us into something that would be like we have a phone, but it would be in our minds.\n",
      " It would be kind of instantaneous.\n",
      " And maybe communication between two people would not require this low bandwidth mechanism of language.\n",
      " Yes, exactly.\n",
      " We don't know what that would be, although we do know that computers can share information like language instantly.\n",
      " They can share many, many books in a second.\n",
      " So we could do that as well.\n",
      " If you look at what our brain does, it actually can manipulate different parameters.\n",
      " So we talk about these large language models.\n",
      " I mean, I had written that it requires a certain amount of information in order to be effective and that we would not see AI really being effective until it got to that level.\n",
      " And we had large language models that were like 10 billion bytes, didn't work very well.\n",
      " They finally got to a hundred billion bytes and now they work fairly well.\n",
      " And now we're going to a trillion bytes.\n",
      " If you say lambda has a hundred billion bytes, what does that mean? Well, what if you had something that had one byte, one parameter, maybe you wanna tell whether or not something's an elephant or not.\n",
      " And so you put in something that would detect its trunk.\n",
      " I didn't, I wasn't inherently a futurist.\n",
      " That was not really my goal.\n",
      " It's really to figure out when things are feasible.\n",
      " We see that now with large scale models.\n",
      " The very large scale models like GPT3, it started two years ago.\n",
      " Four years ago, it wasn't feasible.\n",
      " In fact, they did create GPT2, which didn't work.\n",
      " So it required a certain amount of timing having to do with this exponential growth of computing power.\n",
      " So futurism in some sense is a study of timing, trying to understand how the world will evolve and when will the capacity for certain ideas emerge.\n",
      " And that's become a thing in itself and to try to time things in the future.\n",
      " But really its original purpose was to time my products.\n",
      " I mean, I did OCR in the 1970s because OCR doesn't require a lot of computation.\n",
      " Optical character recognition.\n",
      " Yeah, so we were able to do that in the 70s and I waited till the 80s to address speech recognition since that requires more computation.\n",
      " So you were thinking through timing when you're developing those things.\n",
      " Yeah.\n",
      " Time come.\n",
      " Yeah.\n",
      " And that's how you've developed that brain power to start to think in a futurist sense when how will the world look like in 2045 Thank you for listening, and hope to see you next time.\n",
      " "
     ]
    }
   ],
   "source": [
    "print(remove_timestamps(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c4a10f-81c5-41ae-9de2-9b9899012b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
