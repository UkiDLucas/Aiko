{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f76628c-c4d1-4e17-ba5d-364784792a63",
   "metadata": {},
   "source": [
    "Exploring LangChain\n",
    "https://python.langchain.com/en/latest/getting_started/getting_started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c46e7a-6fef-4ee4-8e9a-734dbd8e56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN ONCE\n",
    "### ! pip install openai ### use conda below\n",
    "# ! conda install langchain -c conda-forge --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a55be82-fb44-4042-953d-1c840000d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8900c69-e3be-481c-b7aa-f41fe1614614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! echo $OPENAI_API_KEY\n",
    "#import os; # os.getenv('OPENAI_API_KEY', 'DEFAULT_VALUE') # print(os.environ)\n",
    "env_OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', 'DEFAULT_VALUE')\n",
    "# print(env_OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77280513-8675-495f-b588-52040f9a8326",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=env_OPENAI_API_KEY) # works without the key, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e4785d-0e5f-46c2-a780-4244a42de9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef6041fa-77ee-48b3-991e-fb2c276e3249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "BrightSteps Socks.\n"
     ]
    }
   ],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "# Cheery Toes. # temperature=0.8\n",
    "# Rainbow Socks Co. # temperature=0.1\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ed07c-adf6-4641-9c97-bfe4021b89c0",
   "metadata": {},
   "source": [
    "# PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a093ed0a-f4de-4049-9fbc-97b5ae3c20d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42777e8e-2367-40d6-9daa-bab0d18db2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a company that makes colorful socks?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(product=\"colorful socks\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e3c10-ed33-4430-a074-4638a347d696",
   "metadata": {},
   "source": [
    "# Chains: Combine LLMs and prompts in multi-step workflows\n",
    "https://python.langchain.com/en/latest/getting_started/getting_started.html#chains-combine-llms-and-prompts-in-multi-step-workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38ed216d-6d64-463c-83cc-e7b12ea257e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4059852-e3d6-4b88-8814-c77d99f4b31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nRainbow Socks Co.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"colorful socks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fdf7f9-1ed4-4141-b609-aa4bf1eafc96",
   "metadata": {},
   "source": [
    "https://python.langchain.com/en/latest/modules/chains/getting_started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512538da-8a7d-4bf4-9bbe-ce02d2ac68da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9202436-db8b-407b-a6d7-921942baf5eb",
   "metadata": {},
   "source": [
    "# Agents: Dynamically Call Chains Based on User Input\n",
    "https://python.langchain.com/en/latest/getting_started/getting_started.html#agents-dynamically-call-chains-based-on-user-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0775577-ae38-4e97-8c0b-c450887854e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install google-search-results -c conda-forge --yes # PackagesNotFoundError\n",
    "# ! pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98ad8618-7ec7-44b0-808c-bb1eb214eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFAULT_VALUE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "env_SERPAPI_API_KEY = os.environ.get('SERPAPI_API_KEY', 'DEFAULT_VALUE')\n",
    "print(env_SERPAPI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "303d33c7-652b-4b24-bede-6132db9bc9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3c7c2a3-93d9-4e70-9343-95e061ac4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's load the language model we're going to use to control the agent.\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ac538-d890-4f75-b24c-28f4bc285940",
   "metadata": {},
   "source": [
    "## The rest of the section does not work because imports failed"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2eeb68f-6bb5-4d6d-adeb-87f4f0bb8e79",
   "metadata": {},
   "source": [
    "# https://python.langchain.com/en/latest/_modules/langchain/agents/load_tools.html\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools(\n",
    "    ['serpapi', 'llm-math'], \n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ad922b6-4dbe-4c9b-a189-99f60676d6cf",
   "metadata": {},
   "source": [
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b37b3b9-0554-42f1-ab28-f11234fe307b",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "https://python.langchain.com/en/latest/modules/callbacks/getting_started.html#callbacks\n",
    "\n",
    "\n",
    "LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.\n",
    "\n",
    "You can subscribe to these events by using the callbacks argument available throughout the API. This argument is a list of handler objects expected to implement one or more of the methods described below in more detail. There are two main callbacks mechanisms:\n",
    "\n",
    "Constructor callbacks will be used for all calls made on that object and will be scoped to that object only, i.e., if you pass a handler to the LLMChain constructor, it will not be used by the model attached to that chain.\n",
    "\n",
    "Request callbacks will be used for that specific request only and all sub-requests it contains (e.g., a call to an LLMChain triggers a call to a Model, which uses the same handler passed through). These are explicitly passed through.\n",
    "\n",
    "Advanced: \n",
    "When you create a custom chain, you can easily set it up to use the same callback system as all the built-in chains. _call, _generate, _run and equivalent async methods on Chains / LLMs / Chat Models / Agents / Tools now receive a 2nd argument called run_manager which is bound to that run and contains the logging methods that can be used by that object (i.e., on_llm_new_token). This is useful when constructing a custom chain. Please take a look at this guide for more information on creating custom chains and using callbacks inside them.\n",
    "\n",
    "CallbackHandlers are objects that implement the CallbackHandler interface, which has a method for each event that can be subscribed to. The CallbackManager will call the appropriate method on each handler when the event is triggered."
   ]
  },
  {
   "cell_type": "raw",
   "id": "faae7ac8-be3e-4dd9-aa60-ad5371c073eb",
   "metadata": {},
   "source": [
    "class BaseCallbackHandler:\n",
    "    \"\"\"Base callback handler that can be used to handle callbacks from langchain.\"\"\"\n",
    "\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM starts running.\"\"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when chain starts running.\"\"\"\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when chain ends running.\"\"\"\n",
    "\n",
    "    def on_chain_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when chain errors.\"\"\"\n",
    "\n",
    "    def on_tool_start(\n",
    "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when tool starts running.\"\"\"\n",
    "\n",
    "    def on_tool_end(self, output: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when tool ends running.\"\"\"\n",
    "\n",
    "    def on_tool_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when tool errors.\"\"\"\n",
    "\n",
    "    def on_text(self, text: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on arbitrary text.\"\"\"\n",
    "\n",
    "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on agent action.\"\"\"\n",
    "\n",
    "    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on agent end.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55784286-2f66-464d-9641-6cb5067797af",
   "metadata": {},
   "source": [
    "# How to use callbacks\n",
    "https://python.langchain.com/en/latest/modules/callbacks/getting_started.html#how-to-use-callbacks\n",
    "\n",
    "The callbacks argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) in two different places:\n",
    "\n",
    "## Constructor callbacks: \n",
    "defined in the constructor, e.g., LLMChain(callbacks=[handler]), which will be used for all calls made on that object and will be scoped to that object only, e.g., if you pass a handler to the LLMChain constructor, it will not be used by the Model attached to that chain.\n",
    "\n",
    "## Request callbacks: \n",
    "defined in the call() run() apply() methods used for issuing a request, e.g. chain.call(inputs, callbacks=[handler]), which will be used for that specific request only, and all sub-requests that it contains (e.g. a call to an LLMChain triggers a call to a Model, which uses the same handler passed in the call() method).\n",
    "\n",
    "The verbose argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) as a constructor argument, e.g., LLMChain(verbose=True), and it is equivalent to passing a ConsoleCallbackHandler to the callbacks argument of that object and all child objects. This is useful for debugging, as it will log all events to the console.\n",
    "\n",
    "## When do you want to use each of these?\n",
    "Constructor callbacks are most useful for use cases such as logging, monitoring, etc., which are not specific to a single request but to the entire chain. \n",
    "For example, if you want to log all the requests made to an LLMChain, you would pass a handler to the constructor.\n",
    "\n",
    "Request callbacks are most useful for use cases such as streaming, where you want to stream the output of a single request to a specific web socket connection, or other similar use cases. For example, if you want to stream the output of a single request to a websocket, you would pass a handler to the call() method\n",
    "\n",
    "## Using an existing handler\n",
    "LangChain provides a few built-in handlers that you can use to get started. These are available in the long chain/callbacks module. The most basic handler is the StdOutCallbackHandler, which logs all events to stdout. In the future, we will add more default handlers to the library.\n",
    "\n",
    "Note when the verbose flag on the object is set to true, the StdOutCallbackHandler will be invoked even without being explicitly passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6154e2a7-d179-4e6c-b4ea-2e4454c9d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "handler = StdOutCallbackHandler()\n",
    "llm = OpenAI()\n",
    "prompt = PromptTemplate.from_template(\"1 + {number} = \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "643653c8-7a08-44b3-b9e6-11da4d3af195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n3'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's explicitly set the StdOutCallbackHandler in `callbacks`\n",
    "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\n",
    "chain.run(number=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99975551-5cc0-4cfa-b2ea-70e08b1386d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n3'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then, let's use the `verbose` flag to achieve the same result\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "chain.run(number=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e819d40-3861-4b34-a71e-8e7f90b05d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n3'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, let's use the request `callbacks` to achieve the same result\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(number=2, callbacks=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf987119-c29c-4130-acb9-ac3408affa14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
